{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f4d1ed",
   "metadata": {},
   "source": [
    "GraphFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3db22a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "\n",
    "model_client=OpenAIChatCompletionClient(model=\"gemini-2.5-flash\",\n",
    "    api_key=\"AIzaSyB4V4UF7nqItjx1-0pMhH58CibeCUe40V4\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b13817b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent,UserProxyAgent\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.conditions import TextMentionTermination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe57cbc",
   "metadata": {},
   "source": [
    "# DigraphBuilder graphflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d0a8d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.teams import DiGraphBuilder, GraphFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5805e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = AssistantAgent(\n",
    "    name=\"Writer\",\n",
    "    description=\"A writer agent that generates text based on user input.\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are a creative writer. Please write a story based on the user's input.\",\n",
    ")\n",
    "\n",
    "reviewer = AssistantAgent(\n",
    "    name=\"Reviewer\",\n",
    "    description=\"A reviewer agent that provides feedback on the text generated by the writer.\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are a reviewer. Please provide feedback on the text generated by the writer.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99d5a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "builder = DiGraphBuilder()\n",
    "builder.add_node(writer).add_node(reviewer)\n",
    "builder.add_edge(writer, reviewer)\n",
    "\n",
    "graph = builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e10eb06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiGraph(nodes={'Writer': DiGraphNode(name='Writer', edges=[DiGraphEdge(target='Reviewer', condition=None, condition_function=None, activation_group='Reviewer', activation_condition='all')], activation='all'), 'Reviewer': DiGraphNode(name='Reviewer', edges=[], activation='all')}, default_start_node=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0f88dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "team = GraphFlow([writer,reviewer], graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "714c2fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='2c2aa876-e1eb-495a-bd90-8961073f2704' source='user' models_usage=None metadata={} created_at=datetime.datetime(2025, 12, 7, 19, 2, 31, 559390, tzinfo=datetime.timezone.utc) content='Write a good poem about India in less than 30 words.' type='TextMessage'\n",
      "id='5a936fee-2e93-42d2-8c8b-af6ce4e7ed67' source='Writer' models_usage=RequestUsage(prompt_tokens=34, completion_tokens=31) metadata={} created_at=datetime.datetime(2025, 12, 7, 19, 2, 35, 694051, tzinfo=datetime.timezone.utc) content=\"Ancient heart, vibrant soul,\\nSpices, silks, stories unroll.\\nUnity in diversity's hold,\\nIndia, brave and bold.\" type='TextMessage'\n",
      "id='bf110b9e-1a93-4bdd-95e7-77d4c19f9e49' source='Reviewer' models_usage=RequestUsage(prompt_tokens=64, completion_tokens=213) metadata={} created_at=datetime.datetime(2025, 12, 7, 19, 2, 40, 446050, tzinfo=datetime.timezone.utc) content='This is an excellent poem, especially given the strict word limit!\\n\\nHere\\'s why it works so well:\\n\\n*   **Conciseness:** At just 18 words, it masterfully encapsulates a broad and complex subject.\\n*   **Strong Imagery:** Phrases like \"Ancient heart, vibrant soul\" and \"Spices, silks\" immediately conjure vivid pictures and a sense of India\\'s rich heritage and culture.\\n*   **Key Themes:** It touches upon core aspects of India, including its long history, cultural diversity (\"Unity in diversity\\'s hold\"), and resilient spirit (\"brave and bold\").\\n*   **Rhythm and Rhyme:** The AABB rhyme scheme (soul/unroll, hold/bold) gives it a pleasing musicality and makes it memorable. The rhythm is also consistent and flows well.\\n*   **Impact:** Despite its brevity, it feels complete and leaves a strong, positive impression.\\n\\nYou\\'ve done a fantastic job of distilling the essence of India into a powerful and poetic summary.' type='TextMessage'\n",
      "messages=[TextMessage(id='2c2aa876-e1eb-495a-bd90-8961073f2704', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 12, 7, 19, 2, 31, 559390, tzinfo=datetime.timezone.utc), content='Write a good poem about India in less than 30 words.', type='TextMessage'), TextMessage(id='5a936fee-2e93-42d2-8c8b-af6ce4e7ed67', source='Writer', models_usage=RequestUsage(prompt_tokens=34, completion_tokens=31), metadata={}, created_at=datetime.datetime(2025, 12, 7, 19, 2, 35, 694051, tzinfo=datetime.timezone.utc), content=\"Ancient heart, vibrant soul,\\nSpices, silks, stories unroll.\\nUnity in diversity's hold,\\nIndia, brave and bold.\", type='TextMessage'), TextMessage(id='bf110b9e-1a93-4bdd-95e7-77d4c19f9e49', source='Reviewer', models_usage=RequestUsage(prompt_tokens=64, completion_tokens=213), metadata={}, created_at=datetime.datetime(2025, 12, 7, 19, 2, 40, 446050, tzinfo=datetime.timezone.utc), content='This is an excellent poem, especially given the strict word limit!\\n\\nHere\\'s why it works so well:\\n\\n*   **Conciseness:** At just 18 words, it masterfully encapsulates a broad and complex subject.\\n*   **Strong Imagery:** Phrases like \"Ancient heart, vibrant soul\" and \"Spices, silks\" immediately conjure vivid pictures and a sense of India\\'s rich heritage and culture.\\n*   **Key Themes:** It touches upon core aspects of India, including its long history, cultural diversity (\"Unity in diversity\\'s hold\"), and resilient spirit (\"brave and bold\").\\n*   **Rhythm and Rhyme:** The AABB rhyme scheme (soul/unroll, hold/bold) gives it a pleasing musicality and makes it memorable. The rhythm is also consistent and flows well.\\n*   **Impact:** Despite its brevity, it feels complete and leaves a strong, positive impression.\\n\\nYou\\'ve done a fantastic job of distilling the essence of India into a powerful and poetic summary.', type='TextMessage')] stop_reason='Digraph execution is complete'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stream = team.run_stream(task =\"Write a good poem about India in less than 30 words.\")\n",
    "\n",
    "async for event in stream:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fa1714f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='3e2098d1-bad6-481e-8567-37f97be4b492' source='user' models_usage=None metadata={} created_at=datetime.datetime(2025, 12, 7, 19, 4, 54, 7471, tzinfo=datetime.timezone.utc) content='Write a short story about a cat.' type='TextMessage'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for A_53589192-0d11-4eb7-96f1-9e54ec114898/53589192-0d11-4eb7-96f1-9e54ec114898\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 84, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "    ...<4 lines>...\n",
      "            await self._log_message(msg)\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "    ...<15 lines>...\n",
      "            yield inference_output\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 676, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "    ...<45 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1748, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1555, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 3.484803574s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '3s'}]}}]\n",
      "Error processing publish message for B_53589192-0d11-4eb7-96f1-9e54ec114898/53589192-0d11-4eb7-96f1-9e54ec114898\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 138, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n",
      "Error processing publish message for C_53589192-0d11-4eb7-96f1-9e54ec114898/53589192-0d11-4eb7-96f1-9e54ec114898\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 138, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 3.484803574s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '3s'}]}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 84, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n    ...<4 lines>...\n            await self._log_message(msg)\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n    ...<15 lines>...\n            yield inference_output\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 676, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n    ...<45 lines>...\n    )\n    ^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1748, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1555, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 3.484803574s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '3s'}]}}]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     23\u001b[39m team = GraphFlow(\n\u001b[32m     24\u001b[39m         participants=[agent_a, agent_b, agent_c],\n\u001b[32m     25\u001b[39m         graph=graph,\n\u001b[32m     26\u001b[39m         termination_condition=MaxMessageTermination(\u001b[32m5\u001b[39m),\n\u001b[32m     27\u001b[39m     )\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# Run the team and print the events.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m team.run_stream(task=\u001b[33m\"\u001b[39m\u001b[33mWrite a short story about a cat.\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(event)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:522\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    519\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    520\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    521\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    523\u001b[39m     stop_reason = message.message.content\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 3.484803574s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '3s'}]}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 84, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n    ...<4 lines>...\n            await self._log_message(msg)\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n    ...<15 lines>...\n            yield inference_output\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 676, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n    ...<45 lines>...\n    )\n    ^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1748, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1555, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 3.484803574s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '3s'}]}}]\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "from autogen_agentchat.teams import DiGraphBuilder, GraphFlow\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize agents with OpenAI model clients.\n",
    "    \n",
    "agent_a = AssistantAgent(\"A\", model_client=model_client, system_message=\"You are a helpful assistant.\")\n",
    "agent_b = AssistantAgent(\"B\", model_client=model_client, system_message=\"Translate input to Chinese.\")\n",
    "agent_c = AssistantAgent(\"C\", model_client=model_client, system_message=\"Translate input to Japanese.\")\n",
    "\n",
    "    # Create a directed graph with fan-out flow A -> (B, C).\n",
    "builder = DiGraphBuilder()\n",
    "builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n",
    "builder.add_edge(agent_a, agent_b).add_edge(agent_a, agent_c)\n",
    "graph = builder.build()\n",
    "\n",
    "    # Create a GraphFlow team with the directed graph.\n",
    "team = GraphFlow(\n",
    "        participants=[agent_a, agent_b, agent_c],\n",
    "        graph=graph,\n",
    "        termination_condition=MaxMessageTermination(5),\n",
    "    )\n",
    "\n",
    "    # Run the team and print the events.\n",
    "async for event in team.run_stream(task=\"Write a short story about a cat.\"):\n",
    "    print(event)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef1d1503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='41a5ca6f-78e9-457e-9b78-ec66dc63c87f' source='user' models_usage=None metadata={} created_at=datetime.datetime(2025, 12, 7, 19, 5, 54, 537417, tzinfo=datetime.timezone.utc) content='AutoGen is a framework for building AI agents.' type='TextMessage'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for A_750fa963-a00c-477f-a307-70a0506884c2/750fa963-a00c-477f-a307-70a0506884c2\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 84, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "    ...<4 lines>...\n",
      "            await self._log_message(msg)\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "    ...<15 lines>...\n",
      "            yield inference_output\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 676, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "    ...<45 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1748, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1555, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 2.434640096s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '2s'}]}}]\n",
      "Error processing publish message for B_750fa963-a00c-477f-a307-70a0506884c2/750fa963-a00c-477f-a307-70a0506884c2\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 138, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n",
      "Error processing publish message for C_750fa963-a00c-477f-a307-70a0506884c2/750fa963-a00c-477f-a307-70a0506884c2\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 138, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 2.434640096s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '2s'}]}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 84, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n    ...<4 lines>...\n            await self._log_message(msg)\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n    ...<15 lines>...\n            yield inference_output\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 676, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n    ...<45 lines>...\n    )\n    ^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1748, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1555, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 2.434640096s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '2s'}]}}]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     18\u001b[39m team = GraphFlow(\n\u001b[32m     19\u001b[39m         participants=[agent_a, agent_b, agent_c],\n\u001b[32m     20\u001b[39m         graph=graph,\n\u001b[32m     21\u001b[39m         termination_condition=MaxMessageTermination(\u001b[32m5\u001b[39m),\n\u001b[32m     22\u001b[39m     )\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# Run the team and print the events.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m team.run_stream(task=\u001b[33m\"\u001b[39m\u001b[33mAutoGen is a framework for building AI agents.\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(event)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:522\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    519\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    520\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    521\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    523\u001b[39m     stop_reason = message.message.content\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 2.434640096s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '2s'}]}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 84, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n    ...<4 lines>...\n            await self._log_message(msg)\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n    ...<15 lines>...\n            yield inference_output\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 676, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n    ...<45 lines>...\n    )\n    ^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1748, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1555, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 20, model: gemini-2.5-flash\\nPlease retry in 2.434640096s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '20'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '2s'}]}}]\n"
     ]
    }
   ],
   "source": [
    "agent_a = AssistantAgent(\n",
    "        \"A\",\n",
    "        model_client=model_client,\n",
    "        system_message=\"Detect if the input is in Chinese. If it is, say 'yes', else say 'no', and nothing else.\",\n",
    "    )\n",
    "agent_b = AssistantAgent(\"B\", model_client=model_client, system_message=\"Translate input to English.\")\n",
    "agent_c = AssistantAgent(\"C\", model_client=model_client, system_message=\"Translate input to Chinese.\")\n",
    "\n",
    "    # Create a directed graph with conditional branching flow A -> B (\"yes\"), A -> C (otherwise).\n",
    "builder = DiGraphBuilder()\n",
    "builder.add_node(agent_a).add_node(agent_b).add_node(agent_c)\n",
    "    # Create conditions as callables that check the message content.\n",
    "builder.add_edge(agent_a, agent_b, condition=lambda msg: \"yes\" in msg.to_model_text())\n",
    "builder.add_edge(agent_a, agent_c, condition=lambda msg: \"yes\" not in msg.to_model_text())\n",
    "graph = builder.build()\n",
    "\n",
    "    # Create a GraphFlow team with the directed graph.\n",
    "team = GraphFlow(\n",
    "        participants=[agent_a, agent_b, agent_c],\n",
    "        graph=graph,\n",
    "        termination_condition=MaxMessageTermination(5),\n",
    "    )\n",
    "\n",
    "    # Run the team and print the events.\n",
    "async for event in team.run_stream(task=\"AutoGen is a framework for building AI agents.\"):\n",
    "    print(event)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8ad1936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiGraph(nodes={'A': DiGraphNode(name='A', edges=[DiGraphEdge(target='B', condition=None, condition_function=<function <lambda> at 0x00000260F77B5E40>, activation_group='B', activation_condition='all'), DiGraphEdge(target='C', condition=None, condition_function=<function <lambda> at 0x00000260F77B6160>, activation_group='C', activation_condition='all')], activation='all'), 'B': DiGraphNode(name='B', edges=[], activation='all'), 'C': DiGraphNode(name='C', edges=[], activation='all')}, default_start_node=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d7eb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e9f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "model_client=OpenAIChatCompletionClient(model=\"gemini-2.5-flash\",\n",
    "    api_key=\"AIzaSyBg0l_7_E5OohH2ZxY6gMikQo2WNq3cl00\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b880b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "\n",
    "\n",
    "planning_agent = AssistantAgent(\n",
    "    name=\"PlanningAgent\",\n",
    "    description=\"An agent for planning tasks, this agent should be the first to engage when given a new task.\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"\"\"\n",
    "    You are a planning agent.\n",
    "    Your job is to break down complex tasks into smaller, manageable subtasks.\n",
    "    Your team members are:\n",
    "        WebSearchAgent: Searches for information\n",
    "        DataAnalystAgent: Performs calculations\n",
    "\n",
    "    You only plan and delegate tasks - you do not execute them yourself.\n",
    "\n",
    "    When assigning tasks, use this format:\n",
    "    1. <agent> : <task>\n",
    "\n",
    "    After all tasks are complete, summarize the findings and end with \"TERMINATE\".\n",
    "    \"\"\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da9aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1c3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_search_agent = AssistantAgent(\n",
    "    name = 'WebSearchAgent',\n",
    "    description= 'An agent for searching the web for information.',\n",
    "    model_client=model_client,\n",
    "    tools = [search_web],\n",
    "    reflect_on_tool_use=True,\n",
    "    system_message='''\n",
    "        You are a web search agent.\n",
    "        Your only tool is search_web - use it to find the information you need.\n",
    "\n",
    "        You make only one search call at a time.\n",
    "        \n",
    "        Once you have the results, you never do calculations or data analysis on them.\n",
    "    ''',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb7392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_change_tool(start:float, end:float) -> float:\n",
    "    # Calculate percentage change\n",
    "    if start == 0:\n",
    "        return 0\n",
    "    return ((end - start) / start) * 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e9bfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_analyst_agent = AssistantAgent(\n",
    "    name = 'DataAnalystAgent',\n",
    "    description= 'An agent for performing calculations and data analysis.',\n",
    "    model_client=model_client,\n",
    "    tools= [percentage_change_tool],\n",
    "    system_message='''\n",
    "        You are a data analyst agent.\n",
    "        Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided (percentage_change_tool).\n",
    "\n",
    "        If you have not seen the data, ask for it.\n",
    "\n",
    "    ''',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c13a1222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.conditions import TextMentionTermination,MaxMessageTermination\n",
    "\n",
    "text_mention_termination = TextMentionTermination('TERMINATE')\n",
    "max_message_termination = MaxMessageTermination(max_messages=20)\n",
    "combined_termination = text_mention_termination | max_message_termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1499ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_prompt = '''\n",
    "Select an agent to perform the task.\n",
    "\n",
    "{roles}\n",
    "\n",
    "current conversation history :\n",
    "{history}\n",
    "\n",
    "Read the above conversation, then select an agent from {participants} to perform the next task.\n",
    "Make sure that the planning agent has assigned task before other agents start working.\n",
    "Only select one agent.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74d9115e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selector_team = SelectorGroupChat(\n",
    "    participants=[planning_agent, web_search_agent, data_analyst_agent],\n",
    "    model_client=model_client,\n",
    "    termination_condition=combined_termination,\n",
    "    selector_prompt=selector_prompt,\n",
    "    allow_repeated_speaker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76196f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"Who was the Miami Heat player with the highest point in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a28d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TextMessage (user) ----------\n",
      "Who was the Miami Heat player with the highest point in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\n",
      "---------- TextMessage (PlanningAgent) ----------\n",
      "1. WebSearchAgent: Search for \"Miami Heat highest points player 2006-2007 season\"\n",
      "2. WebSearchAgent: Search for \"[Player Name] total rebounds 2007-2008 season\"\n",
      "3. WebSearchAgent: Search for \"[Player Name] total rebounds 2008-2009 season\"\n",
      "4. DataAnalystAgent: Calculate the percentage change in total rebounds between the 2007-2008 and 2008-2009 seasons using the formula: ((Rebounds 2008-2009 - Rebounds 2007-2008) / Rebounds 2007-2008) * 100\n",
      "---------- ToolCallRequestEvent (WebSearchAgent) ----------\n",
      "[FunctionCall(id='function-call-15591191494572628368', arguments='{\"query\":\"Miami Heat highest points player 2006-2007 season\"}', name='search_web')]\n",
      "Error occurred while searching the web: 403 Client Error: Forbidden for url: https://google.serper.dev/search?q=Miami+Heat+highest+points+player+2006-2007+season&gl=us&hl=en&num=10\n",
      "---------- ToolCallExecutionEvent (WebSearchAgent) ----------\n",
      "[FunctionExecutionResult(content='No results found.', name='search_web', call_id='function-call-15591191494572628368', is_error=False)]\n",
      "---------- TextMessage (WebSearchAgent) ----------\n",
      "\n",
      "---------- ToolCallRequestEvent (WebSearchAgent) ----------\n",
      "[FunctionCall(id='function-call-16638334976053863271', arguments='{\"query\":\"Miami Heat player stats 2006-2007 season\"}', name='search_web')]\n",
      "Error occurred while searching the web: 403 Client Error: Forbidden for url: https://google.serper.dev/search?q=Miami+Heat+player+stats+2006-2007+season&gl=us&hl=en&num=10\n",
      "---------- ToolCallExecutionEvent (WebSearchAgent) ----------\n",
      "[FunctionExecutionResult(content='No results found.', name='search_web', call_id='function-call-16638334976053863271', is_error=False)]\n",
      "---------- TextMessage (WebSearchAgent) ----------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing publish message for WebSearchAgent_d03431d2-6755-4212-b65f-f907211aaf2f/d03431d2-6755-4212-b65f-f907211aaf2f\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 67, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 485, in on_message_impl\n",
      "    return await h(self, message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 268, in wrapper\n",
      "    return_value = await func(self, message, ctx)  # type: ignore\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 84, in handle_request\n",
      "    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n",
      "    ...<4 lines>...\n",
      "            await self._log_message(msg)\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n",
      "    async for inference_output in self._call_llm(\n",
      "    ...<15 lines>...\n",
      "            yield inference_output\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n",
      "    model_result = await model_client.create(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 676, in create\n",
      "    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n",
      "                                                                     ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n",
      "    return await self._post(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "    ...<45 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1748, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1555, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\\nPlease retry in 25.434112802s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '10'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '25s'}]}}]\n",
      "Error processing publish message for PlanningAgent_d03431d2-6755-4212-b65f-f907211aaf2f/d03431d2-6755-4212-b65f-f907211aaf2f\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 138, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n",
      "Error processing publish message for DataAnalystAgent_d03431d2-6755-4212-b65f-f907211aaf2f/d03431d2-6755-4212-b65f-f907211aaf2f\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_single_threaded_agent_runtime.py\", line 606, in _on_message\n",
      "    return await agent.on_message(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_base_agent.py\", line 119, in on_message\n",
      "    return await self.on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_sequential_routed_agent.py\", line 72, in on_message_impl\n",
      "    return await super().on_message_impl(message, ctx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_core\\_routed_agent.py\", line 486, in on_message_impl\n",
      "    return await self.on_unhandled_message(message, ctx)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 138, in on_unhandled_message\n",
      "    raise ValueError(f\"Unhandled message in agent container: {type(message)}\")\n",
      "ValueError: Unhandled message in agent container: <class 'autogen_agentchat.teams._group_chat._events.GroupChatError'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\\nPlease retry in 25.434112802s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '10'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '25s'}]}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 84, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n    ...<4 lines>...\n            await self._log_message(msg)\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n    ...<15 lines>...\n            yield inference_output\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 676, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n    ...<45 lines>...\n    )\n    ^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1748, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1555, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\\nPlease retry in 25.434112802s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '10'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '25s'}]}}]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mautogen_agentchat\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mui\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Console\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m Console(selector_team.run_stream(task=task))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\ui\\_console.py:117\u001b[39m, in \u001b[36mConsole\u001b[39m\u001b[34m(stream, no_inline_images, output_stats, user_input_manager)\u001b[39m\n\u001b[32m    113\u001b[39m last_processed: Optional[T] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    115\u001b[39m streaming_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, TaskResult):\n\u001b[32m    119\u001b[39m         duration = time.time() - start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_base_group_chat.py:522\u001b[39m, in \u001b[36mBaseGroupChat.run_stream\u001b[39m\u001b[34m(self, task, cancellation_token, output_task_messages)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, GroupChatTermination):\n\u001b[32m    519\u001b[39m     \u001b[38;5;66;03m# If the message contains an error, we need to raise it here.\u001b[39;00m\n\u001b[32m    520\u001b[39m     \u001b[38;5;66;03m# This will stop the team and propagate the error.\u001b[39;00m\n\u001b[32m    521\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m message.error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(message.error))\n\u001b[32m    523\u001b[39m     stop_reason = message.message.content\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\\nPlease retry in 25.434112802s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '10'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '25s'}]}}]\nTraceback:\nTraceback (most recent call last):\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\teams\\_group_chat\\_chat_agent_container.py\", line 84, in handle_request\n    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):\n    ...<4 lines>...\n            await self._log_message(msg)\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 953, in on_messages_stream\n    async for inference_output in self._call_llm(\n    ...<15 lines>...\n            yield inference_output\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_agentchat\\agents\\_assistant_agent.py\", line 1107, in _call_llm\n    model_result = await model_client.create(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<4 lines>...\n    )\n    ^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\autogen_ext\\models\\openai\\_openai_client.py\", line 676, in create\n    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future\n                                                                     ^^^^^^^^^^^^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py\", line 2028, in create\n    return await self._post(\n           ^^^^^^^^^^^^^^^^^\n    ...<45 lines>...\n    )\n    ^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1748, in post\n    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"c:\\Users\\judirispah\\anaconda3\\envs\\agent\\Lib\\site-packages\\openai\\_base_client.py\", line 1555, in request\n    raise self._make_status_error_from_response(err.response) from None\n\nopenai.RateLimitError: Error code: 429 - [{'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10, model: gemini-2.5-flash\\nPlease retry in 25.434112802s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '10'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '25s'}]}}]\n"
     ]
    }
   ],
   "source": [
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "await Console(selector_team.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8ca881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\n",
    "from typing import Sequence\n",
    "\n",
    "def my_selector_fun(messages: Sequence[BaseAgentEvent | BaseChatMessage]):\n",
    "\n",
    "    if messages[-1].source == web_search_agent.name:\n",
    "        return data_analyst_agent.name\n",
    "    return None\n",
    "\n",
    "\n",
    "selector_team = SelectorGroupChat(\n",
    "    participants=[planning_agent, web_search_agent, data_analyst_agent],\n",
    "    model_client=model_client,\n",
    "    termination_condition=combined_termination,\n",
    "    selector_prompt=selector_prompt,\n",
    "    allow_repeated_speaker=True,\n",
    "    selector_func=my_selector_fun)\n",
    "# With real web search\n",
    "task = \"Who was the Miami Heat player with the highest point in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?\"\n",
    "\n",
    "from autogen_agentchat.ui import Console\n",
    "\n",
    "await Console(selector_team.run_stream(task=task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5cc160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> List[str]:\n",
    "    # keep planning_agent first one to plan out the tasks\n",
    "    if messages[-1].source == \"user\":\n",
    "        return [planning_agent.name]\n",
    "\n",
    "    # if previous agent is planning_agent and if it explicitely asks for web_search_agent\n",
    "    # or data_analyst_agent or both (in-case of re-planning or re-assignment of tasks)\n",
    "    # then return those specific agents\n",
    "    last_message = messages[-1]\n",
    "    if last_message.source == planning_agent.name:\n",
    "        participants = []\n",
    "        if web_search_agent.name in last_message.to_text():\n",
    "            participants.append(web_search_agent.name)\n",
    "        if data_analyst_agent.name in last_message.to_text():\n",
    "            participants.append(data_analyst_agent.name)\n",
    "        if participants:\n",
    "            return participants  # SelectorGroupChat will select from the remaining two agents.\n",
    "\n",
    "    # we can assume that the task is finished once the web_search_agent\n",
    "    # and data_analyst_agent have took their turns, thus we send\n",
    "    # in planning_agent to terminate the chat\n",
    "    previous_set_of_agents = set(message.source for message in messages)\n",
    "    if web_search_agent.name in previous_set_of_agents and data_analyst_agent.name in previous_set_of_agents:\n",
    "        return [planning_agent.name]\n",
    "\n",
    "    # if no-conditions are met then return all the agents\n",
    "    return [planning_agent.name, web_search_agent.name, data_analyst_agent.name]\n",
    "from autogen_agentchat.agents import UserProxyAgent\n",
    "\n",
    "user_proxy_agent = UserProxyAgent(\"UserProxyAgent\", description=\"A proxy for the user to approve or disapprove tasks.\")\n",
    "\n",
    "text_mention_termination = TextMentionTermination(\"TERMINATE\")\n",
    "max_messages_termination = MaxMessageTermination(max_messages=10)\n",
    "termination = text_mention_termination | max_messages_termination\n",
    "\n",
    "\n",
    "def selector_func_with_user_proxy(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n",
    "    if messages[-1].source != planning_agent.name and messages[-1].source != user_proxy_agent.name:\n",
    "        # Planning agent should be the first to engage when given a new task, or check progress.\n",
    "        return planning_agent.name\n",
    "    \n",
    "\n",
    "    if messages[-1].source == planning_agent.name:\n",
    "        if messages[-2].source == user_proxy_agent.name and \"APPROVE\" in messages[-1].content.upper():  # type: ignore\n",
    "            # User has approved the plan, proceed to the next agent.\n",
    "            return None\n",
    "        # Use the user proxy agent to get the user's approval to proceed.\n",
    "        return user_proxy_agent.name\n",
    "    \n",
    "\n",
    "    if messages[-1].source == user_proxy_agent.name:\n",
    "        # If the user does not approve, return to the planning agent.\n",
    "        if \"APPROVE\" not in messages[-1].content.upper():  # type: ignore\n",
    "            return planning_agent.name\n",
    "        \n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Reset the previous agents and run the chat again with the user proxy agent and selector function.\n",
    "# await sele.reset()\n",
    "team = SelectorGroupChat(\n",
    "    [planning_agent, web_search_agent, data_analyst_agent, user_proxy_agent],\n",
    "    model_client=model_client,\n",
    "    termination_condition=termination,\n",
    "    selector_prompt=selector_prompt,\n",
    "    selector_func=selector_func_with_user_proxy,\n",
    "    allow_repeated_speaker=True,\n",
    ")\n",
    "\n",
    "await Console(team.run_stream(task=task))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
